{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all ther required libraries for building a model and validation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataframe for final exam\n",
    "df = pd.read_csv(r\"C:\\Users\\srika\\OneDrive\\Documents\\York\\Sem-2 york\\MBAN 6120 - Data Science II\\Assignment 1\\train.csv\",sep=';')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct exploratory analysis on the provided dataset. State your hypothesis based on the insights from your exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12404</th>\n",
       "      <td>36</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>283</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>27</td>\n",
       "      <td>jun</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37032</th>\n",
       "      <td>51</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>516</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>13</td>\n",
       "      <td>may</td>\n",
       "      <td>370</td>\n",
       "      <td>1</td>\n",
       "      <td>363</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35476</th>\n",
       "      <td>27</td>\n",
       "      <td>student</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>8366</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>7</td>\n",
       "      <td>may</td>\n",
       "      <td>458</td>\n",
       "      <td>2</td>\n",
       "      <td>349</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38732</th>\n",
       "      <td>32</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>217</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>15</td>\n",
       "      <td>may</td>\n",
       "      <td>692</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11720</th>\n",
       "      <td>49</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>443</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>20</td>\n",
       "      <td>jun</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age          job  marital  education default  balance housing loan  \\\n",
       "12404   36   technician  married  secondary      no      283     yes  yes   \n",
       "37032   51     services   single  secondary      no      516     yes   no   \n",
       "35476   27      student   single  secondary      no     8366      no   no   \n",
       "38732   32  blue-collar   single  secondary      no      217     yes   no   \n",
       "11720   49   technician   single    unknown      no      443      no   no   \n",
       "\n",
       "        contact  day month  duration  campaign  pdays  previous poutcome    y  \n",
       "12404   unknown   27   jun        56         2     -1         0  unknown   no  \n",
       "37032  cellular   13   may       370         1    363         1  failure   no  \n",
       "35476  cellular    7   may       458         2    349         1    other   no  \n",
       "38732  cellular   15   may       692         3     -1         0  unknown  yes  \n",
       "11720   unknown   20   jun        25        11     -1         0  unknown   no  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
       "       'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
       "       'previous', 'poutcome', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['married', 'single', 'divorced'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'yes':1,'no':0}\n",
    "df['default'] = df['default'].map(dict)\n",
    "df['housing'] = df['housing'].map(dict)\n",
    "df['loan'] = df['loan'].map(dict)\n",
    "df['y'] = df['y'].map(dict)\n",
    "df['marital'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.93621</td>\n",
       "      <td>10.618762</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>45211</td>\n",
       "      <td>12</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>9732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital</th>\n",
       "      <td>45211</td>\n",
       "      <td>3</td>\n",
       "      <td>married</td>\n",
       "      <td>27214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>45211</td>\n",
       "      <td>4</td>\n",
       "      <td>secondary</td>\n",
       "      <td>23202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018027</td>\n",
       "      <td>0.133049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1362.272058</td>\n",
       "      <td>3044.765829</td>\n",
       "      <td>-8019.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>102127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.555838</td>\n",
       "      <td>0.496878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.160226</td>\n",
       "      <td>0.36682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact</th>\n",
       "      <td>45211</td>\n",
       "      <td>3</td>\n",
       "      <td>cellular</td>\n",
       "      <td>29285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.806419</td>\n",
       "      <td>8.322476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>45211</td>\n",
       "      <td>12</td>\n",
       "      <td>may</td>\n",
       "      <td>13766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258.16308</td>\n",
       "      <td>257.527812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>4918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.763841</td>\n",
       "      <td>3.098021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdays</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.197828</td>\n",
       "      <td>100.128746</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>871.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>previous</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580323</td>\n",
       "      <td>2.303441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poutcome</th>\n",
       "      <td>45211</td>\n",
       "      <td>4</td>\n",
       "      <td>unknown</td>\n",
       "      <td>36959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>45211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116985</td>\n",
       "      <td>0.321406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count unique          top   freq         mean          std  \\\n",
       "age        45211.0    NaN          NaN    NaN     40.93621    10.618762   \n",
       "job          45211     12  blue-collar   9732          NaN          NaN   \n",
       "marital      45211      3      married  27214          NaN          NaN   \n",
       "education    45211      4    secondary  23202          NaN          NaN   \n",
       "default    45211.0    NaN          NaN    NaN     0.018027     0.133049   \n",
       "balance    45211.0    NaN          NaN    NaN  1362.272058  3044.765829   \n",
       "housing    45211.0    NaN          NaN    NaN     0.555838     0.496878   \n",
       "loan       45211.0    NaN          NaN    NaN     0.160226      0.36682   \n",
       "contact      45211      3     cellular  29285          NaN          NaN   \n",
       "day        45211.0    NaN          NaN    NaN    15.806419     8.322476   \n",
       "month        45211     12          may  13766          NaN          NaN   \n",
       "duration   45211.0    NaN          NaN    NaN    258.16308   257.527812   \n",
       "campaign   45211.0    NaN          NaN    NaN     2.763841     3.098021   \n",
       "pdays      45211.0    NaN          NaN    NaN    40.197828   100.128746   \n",
       "previous   45211.0    NaN          NaN    NaN     0.580323     2.303441   \n",
       "poutcome     45211      4      unknown  36959          NaN          NaN   \n",
       "y          45211.0    NaN          NaN    NaN     0.116985     0.321406   \n",
       "\n",
       "              min    25%    50%     75%       max  \n",
       "age          18.0   33.0   39.0    48.0      95.0  \n",
       "job           NaN    NaN    NaN     NaN       NaN  \n",
       "marital       NaN    NaN    NaN     NaN       NaN  \n",
       "education     NaN    NaN    NaN     NaN       NaN  \n",
       "default       0.0    0.0    0.0     0.0       1.0  \n",
       "balance   -8019.0   72.0  448.0  1428.0  102127.0  \n",
       "housing       0.0    0.0    1.0     1.0       1.0  \n",
       "loan          0.0    0.0    0.0     0.0       1.0  \n",
       "contact       NaN    NaN    NaN     NaN       NaN  \n",
       "day           1.0    8.0   16.0    21.0      31.0  \n",
       "month         NaN    NaN    NaN     NaN       NaN  \n",
       "duration      0.0  103.0  180.0   319.0    4918.0  \n",
       "campaign      1.0    1.0    2.0     3.0      63.0  \n",
       "pdays        -1.0   -1.0   -1.0    -1.0     871.0  \n",
       "previous      0.0    0.0    0.0     0.0     275.0  \n",
       "poutcome      NaN    NaN    NaN     NaN       NaN  \n",
       "y             0.0    0.0    0.0     0.0       1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all').transpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above : all have medium hence they are all integers but need to check if the continuous or catagorical<br>\n",
    "'age' mean and mediun and almost in the same place.<br>\n",
    "'ana'- is binary<br>\n",
    " 'c_pho' - max value is too high<br>\n",
    " 'diab' - is binary<br>\n",
    " 'e_fract' - mean and median are close together<br>\n",
    " 'hbp' - is binary<br>\n",
    " 'plat' - need to analyse more<br>\n",
    " 's_c' - mean and median are normally distributed<br>\n",
    " 's_s' - mean and median are normally distributed<br>\n",
    " 'sex' - is binary male / female<br>\n",
    " 'smok' - is binary yes/no<br>\n",
    " 'time' - time of death<br>\n",
    " 'DEATH' - our target variable is binary which is good<br>\n",
    "<br>\n",
    "can you see outliers ? what do you understand from 50% and mean.<br>\n",
    "list variables which are catagorical.<br>\n",
    "<br>\n",
    "which variables are continuous.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can you see outliers ? what do you understand from 50% and mean.\n",
    "list variables which are catagorical.\n",
    "\n",
    "which variables are continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 17 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   age        45211 non-null  int64 \n",
      " 1   job        45211 non-null  object\n",
      " 2   marital    45211 non-null  object\n",
      " 3   education  45211 non-null  object\n",
      " 4   default    45211 non-null  int64 \n",
      " 5   balance    45211 non-null  int64 \n",
      " 6   housing    45211 non-null  int64 \n",
      " 7   loan       45211 non-null  int64 \n",
      " 8   contact    45211 non-null  object\n",
      " 9   day        45211 non-null  int64 \n",
      " 10  month      45211 non-null  object\n",
      " 11  duration   45211 non-null  int64 \n",
      " 12  campaign   45211 non-null  int64 \n",
      " 13  pdays      45211 non-null  int64 \n",
      " 14  previous   45211 non-null  int64 \n",
      " 15  poutcome   45211 non-null  object\n",
      " 16  y          45211 non-null  int64 \n",
      "dtypes: int64(11), object(6)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing values:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the total data is around 1050 in Sate date but every other value is missing few details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total count of null are less than 10% hence droping the Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age          0\n",
       "job          0\n",
       "marital      0\n",
       "education    0\n",
       "default      0\n",
       "balance      0\n",
       "housing      0\n",
       "loan         0\n",
       "contact      0\n",
       "day          0\n",
       "month        0\n",
       "duration     0\n",
       "campaign     0\n",
       "pdays        0\n",
       "previous     0\n",
       "poutcome     0\n",
       "y            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for modelling (i.e., data cleaning and feature engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 17 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   age        45211 non-null  int64 \n",
      " 1   job        45211 non-null  object\n",
      " 2   marital    45211 non-null  object\n",
      " 3   education  45211 non-null  object\n",
      " 4   default    45211 non-null  int64 \n",
      " 5   balance    45211 non-null  int64 \n",
      " 6   housing    45211 non-null  int64 \n",
      " 7   loan       45211 non-null  int64 \n",
      " 8   contact    45211 non-null  object\n",
      " 9   day        45211 non-null  int64 \n",
      " 10  month      45211 non-null  object\n",
      " 11  duration   45211 non-null  int64 \n",
      " 12  campaign   45211 non-null  int64 \n",
      " 13  pdays      45211 non-null  int64 \n",
      " 14  previous   45211 non-null  int64 \n",
      " 15  poutcome   45211 non-null  object\n",
      " 16  y          45211 non-null  int64 \n",
      "dtypes: int64(11), object(6)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = pd.get_dummies(df['job'],dtype=int)\n",
    "pd.concat([df,sex],axis=1)\n",
    "df.drop(['sex'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Remarks'] = df['Remarks'].astype('category')\n",
    "df['Gen_new'] = df['Gender'].cat.codes\n",
    "enc = OneHotEncoder()\n",
    "enc_data = pd.DataFrame(enc.fit_transform(df[['Gen_new', 'Rem_new']]).toarray())\n",
    "df = df.join(enc_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].map({'Abnormal':1, 'Normal':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate analysis - first we check the output variable and make sure it's balanced\n",
    "df.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio is\n",
    "60*100 / (122+60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thats decently balanced dataset. so planning to skip smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.astype('int')\n",
    "X_balance,Y_balance = SMOTE().fit_sample(X,Y)\n",
    "X_balance = pd.DataFrame(X_balance, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue = 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sex = df.groupby(['DEATH_EVENT','sex'])[['smoking']].count().unstack()\n",
    "df_sex['total'] = df_sex['smoking'][0] + df_sex['smoking'][1]\n",
    "df_sex['percent_0'] = df_sex['smoking'][0] *100/ df_sex['total']\n",
    "df_sex['percent_1'] = df_sex['smoking'][1] *100/ df_sex['total']\n",
    "df_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['diab', 'y']).size().unstack() / df.groupby('y').size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['ana','DEATH']).size().unstack() / df.groupby('DEATH').size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ver little difference in values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change of data type using astype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['s_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['s_c']>8].index, axis=0,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log transform only to make it more normally distributed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['s_c']=np.log(df['s_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['s_c'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for continuous variables: plot the boxplots with the x-axis as the output variable, and y-axis as the continuous variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3),(ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 5))\n",
    "sns.boxplot(x='target', y='v1', data=df, order=df.groupby('target')['v1'].median().sort_values().index, ax=ax1)\n",
    "sns.boxplot(x='target', y='v1', data=df, order=df.groupby('target')['v1'].median().sort_values().index, ax=ax2)\n",
    "sns.boxplot(x='target', y='v1', data=df, order=df.groupby('target')['v1'].median().sort_values().index, ax=ax3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'age',  'c_pho', 'e_fract' - show signs wiht different death<br>\n",
    "'ana','plat','s_c', 's_s', 'time' - show signs wiht different death <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above : are the variables meaning full for analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "catagorical variable : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['diab','DEATH']).size().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['diab', 'DEATH']).size().unstack() / df.groupby('DEATH').size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above : do they show difference in the output ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Parch'])\n",
    "plt.show()\n",
    "sns.histplot(df['SibSp'])\n",
    "plt.show()\n",
    "sns.histplot(df['Fare'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['c_pho'] > 3000].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['var'] = np.log(df['var'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with PCA to drop features.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "df_std = scaler.fit_transform(df)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(df_std)\n",
    "\n",
    "# The transformed data is an array, convert it back into a dataframe\n",
    "df_pca = pd.DataFrame(df_pca, columns=[f'PC{i+1}' for i in range(len(df.columns))])\n",
    "\n",
    "# Print the cumulative explained variance ratio\n",
    "cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print('Cumulative explained variance ratio:', cumsum_variance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'00'number of varaibles explain the target value by 90%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = df.corr()\n",
    "dc['target'].sort_values(ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "df=df[['target', 'sqft_living', 'grade', 'sqft_living15', 'bathrooms', 'view',\n",
    "       'bedrooms', 'lat', 'waterfront', 'floors', 'yr_renovated', 'renovated',\n",
    "       'yr_built', 'condition', 'long', 'year', 'quarter', 'month', 'age']]\n",
    "plt.figure(figsize=(30, 15))\n",
    "sns.heatmap(df.corr(),annot=True,cmap='viridis', square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.drop('DEATH',axis=1,inplace=True)\n",
    "dc.drop('DEATH',axis=0,inplace=True)\n",
    "dc.replace(1, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index, col_index = np.where(dc.values == dc.max().max())\n",
    "print(dc.index[row_index[0]])\n",
    "print(dc.columns[col_index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(data=df,x ='time',y ='s_c',hue='DEATH')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns based on PCA count. \n",
    "df.drop('DEATH',axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some of the featueres are well corellared with each other and one of them can be selected to represent the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hd_cons'] = df['hd'].map({0:0, 1:1, 2:1, 3:1, 4:1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binning the continuous variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_Age(Age):\n",
    "    if Age > 60:\n",
    "        return \"High\"\n",
    "    elif 40 <= Age <= 60:\n",
    "        return \"Medium 1\"\n",
    "    elif 20 <= Age <= 40:\n",
    "        return \"Medium_2\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "df['Age_cat'] = df['Age'].apply(categorize_Age)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define new metrics (e.g., multiply columns together or create custom categories based on multiple variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['var'] = df['var1']+df['var2']\n",
    "df['var'].diff(1)\n",
    "df['var']=df['var']-1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " adding dummies for all catorical varianbles: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['AGE', 'YEARS_EMPLOYED', 'INCOME', 'INCOME_PER_MEMBER', 'INCOME_PER_CHILD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Age_cat'], prefix='Age',dtype=int)\n",
    "X_encoded = pd.get_dummies(X, columns=cat_columns, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df[num_columns],X_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outlier verification : "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end o f data preprocessing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop('target', axis=1)\n",
    "y=df['Survived']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardizaton & normalization :scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the train and test data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "# num_columns = ['AGE', 'YEARS_EMPLOYED', 'INCOME', 'INCOME_PER_MEMBER', 'INCOME_PER_CHILD']\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit_transform(df[])\n",
    "\n",
    "# norm = MinMaxScalar()\n",
    "# norm.fit_transform(df[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both have 6 columns each so the shape is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 15))\n",
    "sns.scatterplot(x = x_train[:,0],y = x_train[:,1],hue=y_train,style = y_train)\n",
    "plt.show()\n",
    "# Slicing  - all rows, first two high correlated feature columns \n",
    "# for looking at y - spread. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the train data doesnt show any great clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modelling "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a model to predict the output variable. Hint: you will need to try several models to find the best model for your problem.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression, Naive Bayes, KNN, SVM, Decision Tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "log = LogisticRegression()\n",
    "#\n",
    "nb = GaussianNB()\n",
    "#\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "#\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "#\n",
    "svc = SVC(kernel='linear', random_state=123,probability=True)\n",
    "#\n",
    "models = [log, nb, knn, tree, svc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.fit(x_train,y_train)\n",
    "nb.fit(x_train,y_train)\n",
    "knn.fit(x_train,y_train)\n",
    "tree.fit(x_train,y_train)\n",
    "svc.fit(x_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(log, x_train, y_train, cv=10)\n",
    "print(cv_scores)\n",
    "print(cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(nb, x_train, y_train, cv=10)\n",
    "print(cv_scores)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(knn, x_train, y_train, cv=10)\n",
    "print(cv_scores)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(tree,  x_train, y_train, cv=10)\n",
    "print(cv_scores)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(svc, x_train, y_train, cv=10)\n",
    "print(cv_scores)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_log = log.predict_proba(x_test)\n",
    "yhat_nb = nb.predict_proba(x_test)\n",
    "yhat_knn = knn.predict_proba(x_test)\n",
    "yhat_tree = tree.predict_proba(x_test)\n",
    "yhat_svc = svc.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_log = log.predict(x_test)\n",
    "y_pred_nb = nb.predict(x_test)\n",
    "y_pred_knn = knn.predict(x_test)\n",
    "y_pred_tree = tree.predict(x_test)\n",
    "y_pred_svc = svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_log, tpr_log, thresholds_l = roc_curve(y_test, yhat_log[:,1])\n",
    "fpr_nb, tpr_nb, thresholds_nb = roc_curve(y_test, yhat_nb[:,1])\n",
    "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, yhat_nb[:,1]) # * check again\n",
    "fpr_tree, tpr_tree, thresholds_tree = roc_curve(y_test, yhat_tree[:,1])\n",
    "fpr_svc, tpr_svc, thresholds_svc = roc_curve(y_test, yhat_svc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(index = ['accuracy','precision', 'recall'], columns =['logisticReg','NaiveBayes','KNN','Tree','svc']   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.loc['accuracy','logisticReg'] = accuracy_score(y_pred=y_pred_log,y_true=y_test)\n",
    "metrics.loc['precision','logisticReg'] = precision_score(y_pred=y_pred_log,y_true=y_test)\n",
    "metrics.loc['recall','logisticReg'] = recall_score(y_pred=y_pred_log,y_true=y_test)\n",
    "print(confusion_matrix(y_pred=y_pred_log,y_true=y_test))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test,y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.loc['accuracy','NaiveBayes'] = accuracy_score(y_pred=y_pred_nb,y_true=y_test)\n",
    "metrics.loc['precision','NaiveBayes'] = precision_score(y_pred=y_pred_nb,y_true=y_test)\n",
    "metrics.loc['recall','NaiveBayes'] = recall_score(y_pred=y_pred_nb,y_true=y_test)\n",
    "print(confusion_matrix(y_pred=y_pred_nb,y_true=y_test))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.loc['accuracy','KNN'] = accuracy_score(y_pred=y_pred_knn,y_true=y_test)\n",
    "metrics.loc['precision','KNN'] = precision_score(y_pred=y_pred_knn,y_true=y_test)\n",
    "metrics.loc['recall','KNN'] = recall_score(y_pred=y_pred_knn,y_true=y_test)\n",
    "print(confusion_matrix(y_pred=y_pred_knn,y_true=y_test))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test,y_pred_knn))\n",
    "# order of detail inputed is important. Since predictied and actual order can interchange the values of FP & FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.loc['accuracy','Tree'] = accuracy_score(y_pred=y_pred_tree,y_true=y_test)\n",
    "metrics.loc['precision','Tree'] = precision_score(y_pred=y_pred_tree,y_true=y_test)\n",
    "metrics.loc['recall','Tree'] = recall_score(y_pred=y_pred_tree,y_true=y_test)\n",
    "print(confusion_matrix(y_pred=y_pred_tree,y_true=y_test))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test,y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.loc['accuracy','svc'] = accuracy_score(y_pred=y_pred_svc,y_true=y_test)\n",
    "metrics.loc['precision','svc'] = precision_score(y_pred=y_pred_svc,y_true=y_test)\n",
    "metrics.loc['recall','svc'] = recall_score(y_pred=y_pred_svc,y_true=y_test)\n",
    "print(confusion_matrix(y_pred=y_pred_svc,y_true=y_test))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test,y_pred_svc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate each model using the evaluation techniques you have learned in class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting decision tree\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(20,20))\n",
    "feature_names = x_train.columns\n",
    "plot_tree(tree, filled=True, rounded=True, feature_names=feature_names, class_names=['0', '1'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some of the features have been adjusted based on decision tree outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "plt.plot(fpr_log,tpr_log, label='log')\n",
    "plt.plot(fpr_nb,tpr_nb, label='nb')\n",
    "plt.plot(fpr_knn,tpr_knn, label='knn')\n",
    "plt.plot(fpr_tree,tpr_tree, label='tree')\n",
    "plt.plot(fpr_svc,tpr_svc, label='svc')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc scores\n",
    "print('logistic_regression : ',auc(fpr_log,tpr_log))\n",
    "print('NaiveBayes : ',auc(fpr_nb,tpr_nb))\n",
    "print('KNN : ',auc(fpr_knn,tpr_knn))\n",
    "print('Tree : ',auc(fpr_tree,tpr_tree))\n",
    "print('Svc : ',auc(fpr_svc,tpr_svc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------- is best . Doing hyper parameter tuning based off it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10],'solver': ['lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "model = LogisticRegression()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=10, scoring='accuracy')\n",
    "randomsearch = RandomizedSearchCV(model, param_grid, cv=10, n_iter=100, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "randomsearch.fit(x_train, y_train)\n",
    "print(\"Best Hyperparameters grid:\", grid_search.best_params_)\n",
    "print(\"Best Hyperparameters Random:\", randomsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grids for each model\n",
    "log_reg_param_grid = {'C': [0.01, 0.1, 1, 10],'solver': ['lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "knn_param_grid = {'n_neighbors': list(range(1, 20)),'weights': ['uniform', 'distance'],'metric': ['l1', 'euclidean', 'manhattan', 'sokalsneath', 'hamming', 'sokalmichener', 'rogerstanimoto', 'braycurtis', 'dice', 'jaccard', 'pyfunc', 'haversine', 'matching', 'cityblock', 'correlation', 'l2', 'nan_euclidean', 'chebyshev', 'p', 'wminkowski', 'minkowski', 'cosine', 'infinity', 'russellrao', 'seuclidean', 'canberra', 'kulsinski', 'sqeuclidean', 'yule', 'mahalanobis']}\n",
    "decision_tree_param_grid = {'criterion': ['gini', 'entropy'],'max_depth': [None, 10, 20, 30],'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4]}\n",
    "svc_param_grid = {'C': [0.1, 1, 10],'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],'gamma': ['scale', 'auto'],'class_weight': [None, 'balanced']}\n",
    "\n",
    "# Create models\n",
    "log_reg_model = LogisticRegression()\n",
    "naive_bayes_model = GaussianNB()\n",
    "knn_model = KNeighborsClassifier()\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "svc_model = SVC()\n",
    "\n",
    "# Perform RandomizedSearchCV  for each model\n",
    "random_search_log_reg = RandomizedSearchCV(log_reg_model, log_reg_param_grid, cv=10, n_iter=100, scoring='accuracy')\n",
    "random_search_knn = RandomizedSearchCV(knn_model, knn_param_grid, cv=10, n_iter=100, scoring='accuracy')\n",
    "random_search_decision_tree = RandomizedSearchCV(decision_tree_model, decision_tree_param_grid, cv=10, n_iter=100, scoring='accuracy')\n",
    "random_search_svc = RandomizedSearchCV(svc_model, svc_param_grid, cv=10, n_iter=100, scoring='accuracy')\n",
    "\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "grid_search_log_reg = GridSearchCV(log, log_reg_param_grid, cv=10, scoring='accuracy')\n",
    "grid_search_knn = GridSearchCV(knn_model, knn_param_grid, cv=10, scoring='accuracy')\n",
    "grid_search_decision_tree = GridSearchCV(decision_tree_model, decision_tree_param_grid, cv=10, scoring='accuracy')\n",
    "grid_search_svc = GridSearchCV(svc_model, svc_param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "\n",
    "# Fit the models and find best hyperparameters\n",
    "random_search_log_reg.fit(x_train, y_train)\n",
    "random_search_knn.fit(x_train, y_train)\n",
    "random_search_decision_tree.fit(x_train, y_train)\n",
    "random_search_svc.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best parameters from RandomSearch: \", randomsearch_knn.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from both considering random: <br>\n",
    "{'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 30, 'criterion': 'entropy'}<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying the best Hyperparameters:Modle optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = LogisticRegression(**randomsearch.best_params_)\n",
    "cv_scores = cross_val_score(best_model, x_train, y_train, cv=10, scoring='accuracy')\n",
    "best_model.fit(x_train, y_train)\n",
    "# Predict\n",
    "y_hat_best = best_model.predict_proba(x_test)\n",
    "y_pred_best = best_model.predict(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add some details of the output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Check performance metrics: precision, recall, F1 score, roc_auc, accuracy score \n",
    "* If your model isn't performing well, try adding/removing variables, or engineering more features \n",
    "* Once you're confident you have tried everything possible to build the best possible model, you can finalize your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_pred=y_pred_best, y_true=y_test))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Explaining your results \n",
    "* Explain your results by always connecting your decisions to the business problem provided. \n",
    "* Explain your process > how did you decide to clean the data? how do you know that was the best decision? what did you learn from exploratory analysis, and how did this inform your modelling decisions? Which features did you engineer? how did you pick these?  \n",
    "* Explain the final model > which variables were included in the model, and what was the model performance? \n",
    "* Explain how this model can be applied to the future to predict outcomes > how will the business use the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, best_model.predict_proba(x_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=best_model.__class__.__name__)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Explain your model to a business executive. Your explanation should include the following:\n",
    "\n",
    "i. Your data exploration findings, initial hypothesis, and whether the hypothesis proved true.\n",
    "\n",
    "ii. Your approach for building and evaluating the model. \n",
    "\n",
    "iii. The final model you selected and how you know this is the best model for the problem.\n",
    "\n",
    "iv. How the model should be used by the business, in business process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "did you refer notes ? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fir , trankssform, fit transform.\n",
    "closing a model prediction. how to finsih the mdoel.\n",
    "model deplyment.\n",
    "confirmatory data analysis ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
