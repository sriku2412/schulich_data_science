{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is one of the most common and frequently used tools for data analysis. It contains data structures and manipulation tools which make data cleaning, organizing, and analysis easy and efficient. This library is typically used with other libraries like NumPy, SciPy, statsmadels, scikit-learn, and matplotlib.\n",
    "\n",
    "While NumPy works best with homogeneous data, pandas is designed to work with tabular, heterogeneous data. The two primary components of pandas are series and DataFrames. You can think of a series as a column of data, and a DataFrame as a collection of series. You can apply operations to a DataFrame as a whole, or to individual columns – this makes it easy to work with various data types.\n",
    "\n",
    "You can create DataFrames from scratch, but in the real world we often bring data into a DataFrame from another source – for example, a SQL database, or data files like .csv or .xlsx. \n",
    "\n",
    "To install pandas, you can either do so through Anaconda by navigating to your environment and searching through the list of available packages, or directly in Jupyter: \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is typically imported with the alias pd. This is an industry best practice and I recommend you use this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin our practice with pandas by bringing in an external data source - a .csv file. We will use the .csv called 'customers.csv'. You will notice that the general syntax is object = pandas.read_csv(). Pandas also has a read_excel() function which allows you to bring data in from an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have brought the data into a DataFrame, we can start to organize it so it is easy to work with. For starters, we have a column titled 'id'. Let's change the title of this column to something more specific: customer_id.\n",
    "\n",
    "We can do this by using the pandas rename function. The syntax for this is: object.rename(parameters). In our case, we are going to use the columns parameter, and using a dictionary will set the new column name for just the single column we want to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.rename(columns={'id':'customer_id'})\n",
    "df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set our newly defined column as the index of our DataFrame. In thin instance, we are also using the parameter inplace=True which allows us to make this DataFrame change permanently. If we do not include this parameter, or if we set it to False, the change in temporary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.set_index('customer_id', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the head() function to see the first 5 rows of our dataset, and the tail() function to see the bottom 5 rows. If we want to view more rows, we can specify the number of rows we want to see in the brackets. Leaving the brackets blank will only show the first 5 rows. This is a good way to understand what our data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info() function is another useful function for understanding our DataFrame overall. Here, we can see the range of values in the index, the data types for each column, all column headers, and other useful information like how much memory this object is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the index, columns, axes, dtypes descriptors to view different parts of the DataFrame. Index shows us the values in the index column, column shows us the values across the top (column names) and axes shows us both. The dtype descriptor shows us the data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use these descriptors to change the values found within them. For example, if we wanted to change all the column names, we could use the column descriptor instead of the rename() function. However, you will need to list ALL names here even if they are not changing, whereas rename() allows you to change select names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = ['Customers','Num_products','Province','Relationship','Category']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.rename(columns={'Num_products':'Products'})\n",
    "df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the sort_index() function to sort our data in ascending or descending order (denoted by True or False) based on our index value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_index(axis=0, ascending=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sort across the top, by column name, we need to set the axis to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_index(axis=1, ascending=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta obtain the values for a specific column, we can use either the column name as a descriptor, or you can reference the column using the [] square brackets which indicate a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Customers']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a second set of brackets allows you to pull out multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[['Customers', 'Category']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the loc and iloc functions to find specific entries as well. Loc is used to get rows (or columns) with particular labels from the index, whereas iloc gets rows (or columns) at particular positions in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[1002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[[1002,1001]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[:1101,:'Province']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use comparison operators to find records which represent certain values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['Products']==2]\n",
    "#> or <"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['Products']!=2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical operators for Python are |, &, ~ which mean \"or\", \"and\" and \"not\" respectively. The words \"or\", \"and\" and \"not\" don't work Series, because they are effectively used to convert a value to True or False, and it is unclear to Python what it should do. You are not trying to convert anything, simply checking for equivalency, so you need to use one of the logical operators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[(df2['Province']=='AB') | (df2['Province']=='ON')] #list observations where province is AB or ON"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use isin() to find records containing certain information or range of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.index.isin([1004,1101])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['Products'].isin(range(2,4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['Customers'].isin(['Johnny Awesome','Robert Optimus'])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine filters using the & symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[(df2['Products']>2) & (df2['Category'].isin(['wealth','hnw']))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get even more technical, we can use str() funtions to find if specific strings in entries contain certain letters. In the case below, we are first telling Python to reference a location within our DataFrame (using square brackets), then looking into the Customers column and converting all strings into lowercase values (this is because Python is case sensitive and we do not want to write multiple queries to look for lower or upper case letters - don't worry, the change is temporary), and finally we are identifying strings which contain the letter m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.Customers.str.lower().str.contains('m')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter only for those people whose name contains the letter 'm', and choose to display which provinces they live in (without displaying the rest of the columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2.Customers.str.lower().str.contains('m'), ['Province']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's learn how to use drop. We can temporarily drop a column or row using the drop() function. Since we haven't set our inplace=True argument, this is only a temporary change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2.set_index('Customers').drop(['James Dean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sort_values() function we can sort by columns other than the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values('Relationship', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values('Customers', ascending=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to group data together. The groupby function creates a dictionary of new objects for us that we can analyze. It won't actually show you the data in a pivot table format, however you can analyze it as such. I recommend setting the grouped version of your DataFrame as a new object, as this is a permanent change. This way you can access the original DataFrame if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_grouped=df2.groupby('Province')\n",
    "df2_grouped.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_grouped.get_group('ON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_grouped.median()['Products']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby('Province')[['Products','Relationship']].aggregate(['min','mean','max'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next useful insights function we will learn is describe(). This function provides you with all descriptive statistics about each numerical column in your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. To keep things interesting, let's import our second .csv file. This is a transactions file. Let's create a DataFrame and set the index as customer_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = pd.read_csv('transactions.csv')\n",
    "df_trans.set_index('customer_id',inplace=True)\n",
    "df_trans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our customer data and a bunch of transaction data, we can merge the two DataFrames to create one complete DataFrame. For this we will use pandas' merge function. We are going to perform an inner join. This means that we want to join the data only where the values match. In our case here, we are working with fairly clean data sets so this solution is sufficient. In cases where the data is not as clean, we can use outer join funtionalities (left, right, or full) where you will likely have some empty values that can be imputed. You can also use funtions like concat() to stack DataFrames on top of each other. We will cover these methods in the next course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(left=df_trans, right=df2, left_on='customer_id', right_on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our DataFrames are merged, we can perform a number of mathematical operations. Unlike the base Python functions, pandas gives us more to work with - like median()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.groupby('Customers')['txn_total'].aggregate(['min','max','mean'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the isnull() or isna() functions to see if there are any missing values in our data. In the next course, we will learn more about how to handle missing values through various imputation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.isnull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.isna().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sum, pandas makes it very easy to create and manipulate a DataFrame with heterogeneous data, and to perform exploratory analysis on it. This is one of the most common tools you will use when working with data in Python. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
